section,id,question,options,correct,multi,notas,metrics,second_question,second_options,second_correct,second_explanation
Get started with lakehouses in Microsoft Fabric,1,What is the main purpose of a lakehouse in Microsoft Fabric?,To combine data lake flexibility with warehouse-style analytics;To host OLTP transactional systems;To replace Git repositories;To manage email campaigns,1,false,Lakehouses merge lake and warehouse capabilities for analytics on OneLake.,5;3,Which key capability makes a Fabric lakehouse different from a simple file store?;Support for tables over files with SQL access;Support for emails only;Only unstructured blobs;Only virtual machines,1,"A lakehouse exposes both files and tables, enabling SQL analytics on top of OneLake storage.",
Get started with lakehouses in Microsoft Fabric,2,Which items are core components of a Fabric lakehouse?,Files and tables;Dashboards and reports only;Pipelines and notebooks only;Power BI apps only,1,false,Lakehouses organize data into Files and Tables areas.,4;4,Where are these lakehouse items stored?,In OneLake under the workspace;In a local SQL Server database;Only in SharePoint;On a user’s desktop,1,"Lakehouse data is stored in OneLake, Fabric’s unified data lake."
Get started with lakehouses in Microsoft Fabric,3,Which operation can you perform directly in a Fabric lakehouse?,Upload files;Create managed tables;Query tables with SQL;All of the above,4,false,"Lakehouses support file ingestion, table creation, and SQL analytics.",6;1,Through which endpoint can you run T-SQL queries over lakehouse tables?,SQL analytics endpoint;Only Spark;Only Power BI Desktop;Only Excel,1,The SQL analytics endpoint exposes lakehouse tables for T-SQL querying.
Get started with lakehouses in Microsoft Fabric,4,Which scenario is an ideal use case for a lakehouse?,Analytics that combine raw files with curated tables;High-frequency OLTP transaction processing;Email marketing;Version control for code,1,false,Lakehouses are built for analytics scenarios mixing files and structured tables.,6;1,Which user roles are typical consumers of a lakehouse?,"Data engineers, analysts, and data scientists;Only HR staff;Only finance approvers;Only marketing interns",,
Get started with lakehouses in Microsoft Fabric,5,Which action is commonly performed when preparing a new lakehouse?,Create the lakehouse item in a workspace;Create a VM;Install SQL Server on-premises;Configure on-premises clusters manually,1,false,You start by creating a lakehouse item in a Fabric workspace.,5;3,After creating a lakehouse, which is a common next step?,Upload or ingest data into Files and Tables;Configure email alerts only;Create Teams channels;Install local drivers,1
Use Apache Spark in Microsoft Fabric,1,What does Apache Spark provide in Microsoft Fabric?,Distributed processing for large-scale data analytics;Only simple single-threaded queries;Email handling;Dashboard theming,1,false,Spark offers distributed compute for big data analytics over lakehouse data.,4;0,Where does Spark typically read and write data in Fabric?,Lakehouse files and tables;Local temp folders only;Exchange mailboxes;SharePoint calendars,1,Spark integrates with lakehouse storage (files and Delta tables) in OneLake.
Use Apache Spark in Microsoft Fabric,2,Which artifact do you use to write interactive Spark code in Fabric?,Spark notebooks;PowerPoint slides;Email templates;Pipeline templates,1,false,Spark notebooks let you author and run interactive code against Spark clusters.,4;0,Which languages are typically supported in Fabric Spark notebooks?,PySpark, Scala, SQL
Use Apache Spark in Microsoft Fabric,3,Which Spark abstraction is most commonly used to represent tabular data in memory?,Dataframe;Stored procedure;Email thread;Pivot chart,1,false,"Spark dataframes represent distributed, tabular data for processing.",3;1,How can you persist a dataframe as a table in a lakehouse?,Write it as a Delta table to the lakehouse;Paste it into Excel;Send it by email;Screenshot the notebook,1,You typically write dataframes as Delta tables stored in the lakehouse.
Use Apache Spark in Microsoft Fabric,4,Which scenario is a good fit for using Spark in Fabric?,Processing large volumes of data with complex transformations;Sending small ad-hoc queries to OLTP;Designing slide decks;Managing users in Entra ID,1,false,"Spark is well suited for large-scale, compute-intensive transformations.",4;0,Which type of workload benefits most from Spark’s distributed nature?,Big data ETL and advanced analytics;Single-row updates only;Manual file renames;Inbox triage,1,Distributed compute helps handle big data ETL and analytical workloads efficiently.
Use Apache Spark in Microsoft Fabric,5,Which feature in Fabric allows you to schedule non-interactive Spark workloads?,Spark job definitions;PowerPoint animations;Excel macros;Manual notebook copy,1,false,Spark job definitions let you operationalize Spark workloads.,4;0,How do you typically orchestrate Spark jobs alongside other activities?,Use Data Factory pipelines;Send emails;Create Teams meetings;Print run logs,1,Pipelines orchestrate Spark jobs together with other data movement and transformation tasks.
Work with Delta Lake tables in Microsoft Fabric,1,What storage format do Fabric lakehouse tables use by default?,Delta;XML;Plain text;Binary blobs without metadata,1,false,Lakehouse tables are stored as Delta Lake tables.,4;0,Which underlying file format does Delta build upon?,Parquet;CSV-only;XLSX;BMP,1,Delta extends Parquet with transaction logs and metadata.
Work with Delta Lake tables in Microsoft Fabric,2,Which capabilities are key benefits of using Delta tables?,ACID transactions;Schema enforcement and evolution;Time travel;All of the above,4,false,"Delta combines ACID, schema management, and time travel features.",3;1,Why are ACID guarantees important for analytics tables?,They ensure consistent reads and writes;They only affect UI themes;They only speed up emails;They disable logging,1,ACID guarantees protect data consistency during concurrent reads and writes.
Work with Delta Lake tables in Microsoft Fabric,3,Which operation creates a Delta table from a Spark dataframe?,Write the dataframe using a Delta format to the lakehouse path;Save as plain CSV only;Copy-paste into a notebook cell;Attach as an email,1,false,You write the dataframe as a Delta table into the lakehouse storage.,4;0,Which Spark command type is often used to define a Delta table with SQL?,CREATE TABLE USING DELTA;CREATE XML TABLE;CREATE CSV-ONLY TABLE;CREATE EMAIL TABLE,1,Spark SQL supports creating tables that use the DELTA format.
Work with Delta Lake tables in Microsoft Fabric,4,Which technique helps optimize Delta tables in Fabric?,Optimize file layout and run VACUUM;Store everything in one large file;Disable statistics;Use only XML,1,false,Optimize and VACUUM improve performance and manage retained history.,4;0,Why is small-file compaction important?,Reduces overhead and improves query performance;Only increases storage;Only affects UI;Disables time travel,1,"Compaction reduces the number of tiny files, improving query efficiency."
Work with Delta Lake tables in Microsoft Fabric,5,How can you query historical versions of a Delta table?,Use time travel with version or timestamp;Edit files manually;Use email history;Restore from screenshots,1,false,Delta time travel lets you query by version or timestamp.,4;0,Which scenario best uses time travel?,Auditing past states of data;Changing UI themes;Configuring licenses;Managing calendars,1,"Time travel is ideal for audits, debugging, and reproducible experiments."
Ingest Data with Dataflows Gen2 in Microsoft Fabric,1,What are Dataflows Gen2 primarily used for in Fabric?,Visually ingesting and transforming data;Managing users;Sending email;Creating PowerPoint slides,1,false,"Dataflows Gen2 provide Power Query-based, visual data ingestion and transformation.",6;0,Which technology underlies Dataflows transformations?,Power Query (M);T-SQL only;Python only;XML only,1,"Dataflows are built on Power Query, using the M language for transformations."
Ingest Data with Dataflows Gen2 in Microsoft Fabric,2,Which sources can Dataflows typically connect to?,Cloud data sources;On-premises sources via gateway;Files in OneLake;All of the above,4,false,"Dataflows support many connectors, including cloud, on-premises via gateway, and OneLake files.",6;0,Where can the output of a Dataflow Gen2 be stored in Fabric?,Lakehouse tables;Datamarts or warehouses;Other Fabric items depending on connector;All of the above,4,"Dataflows can write to multiple Fabric destinations, including lakehouses and warehouses."
Ingest Data with Dataflows Gen2 in Microsoft Fabric,3,Which of the following scenarios is best suited for Dataflow Gen2 rather than a Spark notebook?,Visual data preparation without coding;Advanced machine learning and data science;Direct manipulation of file systems;Managing compute clusters,1,false,"Dataflows are best for visual, business-friendly data shaping.",6;0,Where are Dataflows typically authored?,In the Data Factory hub using Power Query Online;Only in Visual Studio;Only via CLI;Only in Excel,1,
Ingest Data with Dataflows Gen2 in Microsoft Fabric,4,How can you operationalize a Dataflow Gen2 as part of a broader solution?,Add the Dataflow activity to a pipeline;Send a manual email reminder;Run it only ad-hoc;Embed it in PowerPoint,1,false,Pipelines can trigger and orchestrate Dataflows as activities.,6;0,What is a common reason to include a Dataflow in a pipeline?,Schedule and chain it with other ingestion tasks;Change UI colors;Manage licenses;Reset passwords,1,Pipelines schedule Dataflows and coordinate them with other activities.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,5,Which best practice applies when designing Dataflows Gen2?,Apply meaningful naming and reuse queries where possible;Use random names;Avoid documentation;Hardcode all values,1,false,Good naming and reusable queries improve maintenance and governance.,6;0,How can parameters help in Dataflows?,Make queries reusable across environments or filters;Only change fonts;Only manage licenses;They have no use,1,Parameters allow dynamic behavior and reuse of the same Dataflow logic.
Orchestrate processes and data movement with Microsoft Fabric,1,What is the main purpose of a pipeline in Microsoft Fabric?,Orchestrate data movement and transformation;Design slide decks;Manage user licenses;Send newsletters,1,false,Pipelines orchestrate and schedule data-related activities.,6;0,Which typical activities can a pipeline run?,Copy Data;Dataflows;Notebooks or Spark jobs;All of the above,4,"Pipelines coordinate different activities such as Copy Data, Dataflows, and notebooks."
Orchestrate processes and data movement with Microsoft Fabric,2,What does the Copy Data activity do?,Moves or copies data between sources and destinations;Changes UI themes;Sends emails;Manages security roles,1,false,Copy Data is used to ingest or move data from a source to a sink.,6;0,Which scenario is ideal for Copy Data?,Bulk load from a database to a lakehouse;Editing report visuals;Sending notifications;Managing meetings,1,Copy Data is commonly used for bulk ingestion into OneLake or warehouses.
Orchestrate processes and data movement with Microsoft Fabric,3,Which feature helps you quickly create a pipeline for common scenarios?,Pipeline templates;Email templates;Teams bots;Excel macros,1,false,Templates provide pre-built patterns for common data ingestion tasks.,5;1,Why are templates useful for new users?,They reduce setup time and provide best-practice patterns;They slow work;They remove monitoring;They disable security,1,Templates guide new users through recommended configurations.
Orchestrate processes and data movement with Microsoft Fabric,4,How can you monitor pipeline executions in Fabric?,Use the monitoring view in Data Factory;Check email spam;Use file explorer;Ask verbally,1,false,"Data Factory offers run history, status, and details for pipeline executions.",5;0,Which metrics are important to review when monitoring pipelines?,Success/failure status and duration;Slide count;Unread emails;Desktop icons,1,Status and duration help identify issues and performance bottlenecks.
Organize a Fabric lakehouse using medallion architecture design,1,What is the primary idea behind medallion architecture in a lakehouse?,Organize data into progressive quality layers (Bronze - Silver - Gold);Store everything in one folder;Use only CSV;Focus only on dashboards;Mix all data types randomly,1,false,Medallion architecture structures data into layered quality stages.,4;0,Which layer typically holds raw or minimally processed data?,Bronze;Silver;Gold;Dashboard;Archive,1,Bronze holds raw data.
Organize a Fabric lakehouse using medallion architecture design,2,Which layer is usually responsible for cleaned and conformed data?,Bronze;Silver;Gold;Archive;Raw,2,false,"Silver stores cleaned, standardized, and conformed data.",4;0,Which layer is most often used directly by reports and semantic models?,Bronze;Silver;Gold;Inbox;Staging,3,"Gold holds curated, business-ready data for reporting."
Organize a Fabric lakehouse using medallion architecture design,3,Which benefit is a key motivation for using medallion architecture?,Improved data quality and governance as data moves through layers;More silos;Less transparency;Random file structures;Increased complexity,1,false,"Layering improves quality, governance, and clarity of data transformations.",4;0,Which practice supports good governance in medallion design?,Clearly naming folders - tables per layer;Using random names;Mixing dev and prod;Hiding documentation;No documentation,1,Clear naming per layer aids understanding and governance.
Organize a Fabric lakehouse using medallion architecture design,4,How can Power BI take advantage of medallion architecture in Fabric?,Connect semantic models to Gold (and sometimes Silver) tables;Read only raw Bronze files;Ignore layers;Use only CSV exports;Directly query Bronze,1,false,Power BI semantic models typically connect to curated Gold tables.,4;0,Which connectivity mode can provide low-latency access to lakehouse data?,Direct Lake;Only import from CSV;XML feed;Email attachments;Manual refresh,1,Direct Lake can read lakehouse tables directly from OneLake for fast analytics.
Get started with lakehouses in Microsoft Fabric,6,How does a lakehouse relate to a traditional data warehouse in Fabric?,A lakehouse can act as a source for a warehouse;A lakehouse replaces all warehouses;A warehouse stores only files;They cannot interact,1,false,Lakehouses often feed curated data into warehouses when needed.,5;3,Which endpoint is optimized for large-scale relational reporting?,Warehouse SQL endpoint;Lakehouse SQL endpoint;KQL only;Power BI only,1,Warehouses provide dedicated relational storage optimized for reporting workloads.
Get started with lakehouses in Microsoft Fabric,7,Which workspace concept is important when organizing lakehouses?,Use workspaces as environment and security boundaries;Use a single workspace for everything;Avoid separating dev and prod;Use personal workspaces only,1,false,Workspaces act as containers and security scopes for lakehouses.,6;2,How is access to a lakehouse typically controlled?,Through workspace and item permissions;By local file ACL only;By email lists;By PowerPoint roles,1,Permissions at workspace and item level control who can access the lakehouse.
Get started with lakehouses in Microsoft Fabric,8,Which statement best describes OneLake?,"A single, logical data lake for all Fabric items;A separate lake per workspace;An on-premises file share;A Power BI-only storage",1,false,0;1,6;1,What is a key advantage of OneLake for lakehouses?,Eliminating data silos;Increasing isolated copies;Requiring manual sync;Removing metadata,1,
Get started with lakehouses in Microsoft Fabric,9,Which action is appropriate when adding external data to a lakehouse without copying it?,Create a shortcut to the external location;Manually upload the files each time;Take screenshots;Use email attachments,1,false,Shortcuts virtualize external data into the lakehouse.,5;2,What is a benefit of using shortcuts over copying data?,Less duplication and simpler governance;More copies;More manual steps;No security control,1,Shortcuts reduce duplication and centralize governance over shared datasets.
Get started with lakehouses in Microsoft Fabric,10,When would you choose a lakehouse instead of storing data only in a KQL database?,When you need both file-based and tabular analytics;When you only run log queries;When you only store metrics;When you only need dashboards,1,false,Lakehouses are better when combining files and tables for analytics.,6;1,Which engine is more focused on time-series log analytics?,KQL database;Lakehouse SQL;Spark only;Excel only,1,KQL databases are optimized for log and time-series analytics.
Get started with lakehouses in Microsoft Fabric,11,Which operation can be done from the “Tables” area in a lakehouse?,Create new tables;View table schemas;Preview data;All of the above,4,false,The Tables area lets you manage and explore lakehouse tables.,6;1,Which file format backs most lakehouse tables?,Delta over Parquet;XLSX;TXT;BMP,1,Lakehouse tables are implemented as Delta tables over Parquet files.
Get started with lakehouses in Microsoft Fabric,12,Which scenario is a poor fit for a lakehouse?,High-throughput OLTP transaction processing;Analytical workloads mixing files and tables;Data science model training;Ad-hoc SQL analytics,1,false,Lakehouses are not intended for OLTP transactional workloads.,3;5,Where should OLTP workloads typically live?,Operational databases or OLTP systems;Lakehouse only;Power BI only;SharePoint lists only,1,"OLTP scenarios belong in operational databases, not lakehouses."
Get started with lakehouses in Microsoft Fabric,13,Which practice helps keep a lakehouse maintainable over time?,Using clear naming conventions for folders and tables;Using random names;Mixing prod and test data;Storing undocumented files,1,false,Naming conventions improve discoverability and governance.,6;1,How can documentation be attached to a lakehouse design?,Use workspace wikis or README files;Only verbal explanations;Only email threads;No documentation,1,"Written docs describe lakehouse purpose, structure, and usage."
Get started with lakehouses in Microsoft Fabric,14,Which connectivity option can Power BI use to read data from a lakehouse?,Direct Lake;DirectQuery to warehouse only;Export to CSV only;Import from XML only,1,false,Direct Lake can connect Power BI models directly to lakehouse tables.,6;1,Which layer of data is usually best for Power BI models?,Curated tables prepared for analytics;Raw Bronze files only;Temporary staging folders;Random CSV dumps,1,"Curated, analytics-ready tables provide the best source for semantic models."
Use Apache Spark in Microsoft Fabric,6,How are Spark sessions provided in Fabric?,As serverless Spark compute managed by Fabric;As on-premises clusters only;As user-installed VMs;As browser extensions,1,false,"Fabric provides managed, serverless Spark sessions.",3;0,What is a benefit of serverless Spark?,No manual cluster provisioning or management;Manual VM setup;Fixed capacity only;No autoscaling,1,Serverless Spark removes the need to manage clusters directly.
Use Apache Spark in Microsoft Fabric,7,Which command style is commonly used to explore data quickly in Spark notebooks?,Spark SQL queries;PowerPoint animations;XML configuration;Email replies,1,false,Spark SQL provides a convenient way to explore dataframes and tables.,3;0,How can you register a dataframe as a temporary view for SQL queries?,Create or replace temp view over the dataframe;Email the dataframe;Copy to clipboard;Export to PNG,1,Temporary views let you query dataframes with Spark SQL.
Use Apache Spark in Microsoft Fabric,8,Which operation is recommended to efficiently read Delta tables from a lakehouse?,Use Spark’s Delta reader and partition pruning;Load all files as text;Ignore partitions;Use screenshots,1,false,Using native Delta readers with partition pruning improves performance.,4;2,Why is pushdown and pruning important?,Reduces data scanned and speeds up queries;Increases IO;Disables caching;Removes metadata,1,"Pruning limits what data is read, improving performance."
Use Apache Spark in Microsoft Fabric,9,Which practice helps avoid unnecessary Spark cluster costs?,Stop or let sessions end when not in use;Keep sessions running idle;Run heavy jobs repeatedly without need;Duplicate clusters,1,false,Releasing compute when idle reduces cost.,3;0,Where can you see Spark job and session usage?,Monitoring and run history views;Only via email;Only via local logs;Not available,1,Monitoring views show Spark job history and resource usage.
Use Apache Spark in Microsoft Fabric,10,Which type of workload is better done in Spark than in Dataflows Gen2?,Complex code-heavy data science and ML pipelines;Simple no-code data shaping;Small Excel-style transforms;Email parsing,1,false,Spark is ideal for advanced analytics and ML workloads.,3;0,Which Spark language is commonly used for ML libraries?,PySpark;HTML;XAML;Markdown only,1,
Use Apache Spark in Microsoft Fabric,11,Which approach helps ensure Spark code is reusable and maintainable?,Modularize logic into functions and notebooks under source control;Keep all logic in one long cell;Use only ad-hoc edits;Avoid version control,1,false,"Modular, version-controlled notebooks are easier to maintain.",2;0,Where can you store notebook code for versioning?,Git repositories;Email drafts;Local temp only;Clipboard,1,Storing notebooks in Git enables version control.
Use Apache Spark in Microsoft Fabric,12,Which is a good pattern for writing results from Spark to a lakehouse?,Write to staging tables or path then promote;Overwrite Gold directly with no validation;Write only to local temp;Only cache data in memory,1,false,Using staging then promotion reduces risk from failed jobs.,2;0,What should you do before promoting data to curated tables?,Validate and test the data;Skip checks;Delete old datasets blindly;Rely only on emails,1,
Use Apache Spark in Microsoft Fabric,13,How can Spark integrate with pipelines in Fabric?,Pipelines can run notebook or Spark job activities;Spark cannot be orchestrated;Only manual runs;Only Power BI triggers,1,false,Pipelines orchestrate Spark activities as part of broader workflows.,2;0,Which scenario suits a pipeline-triggered Spark job?,Scheduled nightly transformations;Single local test only;Manual file copy;Slide design,1,Pipelines reliably schedule Spark transformations at defined times.
Use Apache Spark in Microsoft Fabric,14,Which option helps optimize Spark performance over lakehouse data?,Co-locating data in OneLake and using Delta;Copying data to local disks;Using XML only;Avoiding partitions,1,false,Delta on OneLake with partitioning and caching improves Spark performance.,2;0,Which caching feature can speed up repeated Spark queries?,In-memory caching of dataframes or tables;Only disk defragmentation;Email cache;PowerPoint cache,1,Caching hot data reduces repeated IO from storage.
Work with Delta Lake tables in Microsoft Fabric,6,Which operation is commonly used to upsert data into a Delta table?,MERGE using a key;Only INSERT;Only UPDATE without key;Manual file edits,1,false,"MERGE lets you insert, update, or delete rows based on keys.",3;1,Which column type is often used as a key in MERGE?,Business or surrogate key;Random GUID for each read;File path only;Email ID,1,MERGE typically matches on business or surrogate keys.
Work with Delta Lake tables in Microsoft Fabric,7,Which feature of Delta supports handling late-arriving data?,ACID merges and schema persistence;Ignoring late data;Editing Parquet files manually;Deleting entire tables,1,false,Delta’s ACID operations allow safe upserts of late-arriving rows.,4;0,Which design helps manage late-arriving facts?,Use proper keys and load dates;Avoid keys;Mix schemas;Use only text logs,1,Keys and load metadata help integrate late-arriving data correctly.
Work with Delta Lake tables in Microsoft Fabric,8,Which statement best describes Delta’s transaction log?,A log that tracks all table changes and metadata;An email log;A UI theme file;A PowerPoint index,1,false,The transaction log records operations for Delta tables.,4;0,Why is the transaction log important?,It enables ACID, time travel, and recovery;It only changes fonts;It deletes data;It disables security
Work with Delta Lake tables in Microsoft Fabric,9,Which practice helps manage long-term storage of Delta tables?,Use VACUUM to clean old files per retention policy;Delete all logs daily;Never remove old versions;Use manual file deletes,1,false,VACUUM removes obsolete files according to retention settings.,4;0,What is a risk of misconfiguring VACUUM retention too low?,Accidentally deleting files needed for time travel or recovery;Improving auditability;Increasing history;Enabling backups,1,Too-low retention can remove data needed for historical queries.
Work with Delta Lake tables in Microsoft Fabric,10,Which column is commonly used for partitioning Delta tables in a lakehouse?,Date or timestamp columns;Free-text description;Random GUID;Email subject,1,false,Partitioning on dates is common for large analytical tables.,4;0,What happens if you over-partition a table?,Too many small files can hurt performance;It always speeds queries;It prevents time travel;It disables ACID,1,Too many tiny partitions create small-file issues and overhead.
Work with Delta Lake tables in Microsoft Fabric,11,Which is a best practice for schema changes in Delta?,Plan and apply controlled schema evolution;Change files manually;Ignore column types;Drop and recreate tables blindly,1,false,Controlled schema evolution avoids breaking consumers.,4;0,Which feature helps add new columns safely?,Delta’s schema evolution options;XML updates;Email notifications;Clipboard history,1,Delta can add new columns while preserving existing data.
Work with Delta Lake tables in Microsoft Fabric,12,Which access methods can read Delta tables in Fabric?,Spark;SQL endpoint over lakehouse;Power BI via Direct Lake;All of the above,4,false,Multiple engines can read Delta tables in Fabric.,3;1,Why is this multi-engine support valuable?,Same data can serve many workloads;Forces data copies;Prevents reuse;Only helps emails,1,"One Delta table can serve Spark, SQL, and BI without duplication."
Work with Delta Lake tables in Microsoft Fabric,13,Which scenario is best suited for Delta over simple Parquet?,Frequent updates and upserts;Write-once read-rarely;Static archive only;Image storage only,1,false,Delta excels where tables change over time and need ACID.,4;1,Which cost can be reduced by using Delta features like OPTIMIZE?,Query time and IO;License fees only;Email volume;UI load time,1,
Work with Delta Lake tables in Microsoft Fabric,14,Which kind of errors can Delta help mitigate?,Dirty reads and partial writes;UI color issues;Email spam;Clipboard loss,1,false,ACID properties reduce issues like partial writes and inconsistent reads.,4;0,Which feature helps ensure only committed changes are visible?,Transactional commits in the Delta log;Manual file saves;PowerPoint autosave;Clipboard flush,1,Delta’s transactional model exposes only committed changes.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,6,Which refresh modes are typically available for Dataflows Gen2?,On-demand and scheduled refresh;Only manual;Only continuous;Only once,1,false,Dataflows can run on demand or via scheduled refresh.,6;0,Where do you configure Dataflow refresh schedules?,In the Dataflow settings;Only in PowerPoint;In Outlook;On the desktop,1,Schedules are configured in Dataflow settings in Fabric.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,7,Which feature helps create reusable logic inside Dataflows?,Referenced queries and parameters;Copy-paste everything;Hardcode all values;Use random names,1,false,Referenced queries and parameters promote reuse and maintainability.,6;0,What is one use of parameters in a Dataflow?,Switching environments or filters without changing logic;Changing fonts;Managing licenses;Sending emails,1,Parameters externalize variables for reuse across environments.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,8,Which scenario is better served by Dataflows than by Spark?,Citizen developers doing self-service data prep;Complex ML pipelines;Low-level file IO;Cluster administration,1,false,Dataflows target self-service and low-code data preparation.,6;0,Which language must a citizen developer know to use Dataflows?,Basic Power Query concepts in a visual UI;Complex Scala;C++;COBOL,1,The main skill is understanding Power Query transformations in the UI.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,9,Which capability helps with incremental ingestion in Dataflows Gen2?,Using incremental refresh with key and date fields;Always full reload;Manual file copy;Email-based updates,1,false,Incremental refresh reduces data volume processed each run.,6;0,What is a benefit of incremental refresh?,Lower compute and faster refreshes;More data scanned;Slower updates;No control,1,Incremental refresh improves performance and efficiency.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,10,Which practice improves Dataflow performance?,Filter early and remove unnecessary columns;Push all filters to the end;Always expand everything;Use many nested joins without need,1,false,Early filtering and column reduction minimize data processed.,6;0,Why is query folding important in Dataflows?,It pushes work to the source for efficiency;It always slows queries;It removes security;It disables refresh,1,Query folding lets the source system handle heavy operations.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,11,How can Dataflows and lakehouses work together?,Dataflows load curated data into lakehouse tables;Lakehouses control Dataflow UI;Dataflows only export CSV;They cannot integrate,1,false,Dataflows can populate lakehouse tables as part of ingestion.,5;0,Which layer is a common target for Dataflows in a medallion design?,Bronze or Silver, depending on use;Gold only;Dashboard;Inbox,1
Ingest Data with Dataflows Gen2 in Microsoft Fabric,12,Which observation is correct about Dataflow error handling?,You can review error details for failed refreshes;Errors are invisible;Only emails show issues;No logs exist,1,false,Dataflows provide error details and logs in the UI.,5;1,Why is monitoring Dataflows important?,To detect and fix ingestion issues quickly;To add more slides;To change themes;To rename users,1,Monitoring ensures data pipelines remain healthy and reliable.
Ingest Data with Dataflows Gen2 in Microsoft Fabric,13,Which is a good way to separate dev and prod for Dataflows?,Use different workspaces or parameterized connections;Edit prod directly;Use one Dataflow for all;Rely on local files,1,false,Separate workspaces or parameters help manage environments safely.,5;0,What is a common pattern for promoting changes?,Develop in dev, test, then replicate or parameterize in prod;Change prod first;Ignore testing;Copy random files
Ingest Data with Dataflows Gen2 in Microsoft Fabric,14,Which output mode can preserve transformations in a reusable way?,Load into tables or destinations and reuse them downstream;Only export to Excel;Only show preview;Send screenshots,1,false,Persisting Dataflow outputs to tables makes them reusable for other workloads.,5;0,Who typically consumes Dataflow outputs?,Other Dataflows, lakehouses, warehouses
Orchestrate processes and data movement with Microsoft Fabric,5,Which triggers can start a pipeline in Fabric?,Manual (on-demand) and scheduled triggers;Only manual;Only API;Only email,1,false,Pipelines support on-demand and scheduled triggers.,5;0,Why are schedules useful for data pipelines?,Automate recurring loads without manual intervention;Reduce automation;Block access;Change themes,1,Schedules ensure data loads happen at consistent times.
Orchestrate processes and data movement with Microsoft Fabric,6,Which pipeline activity type controls flow based on conditions?,If Condition activity;Copy Data activity;Notebook activity;Spark job definition,1,false,If Condition allows branching based on expressions.,5;0,Which activity can repeat actions for a collection of items?,ForEach activity;Copy Data only;Email activity;None,1,ForEach iterates over arrays or collections in a pipeline.
Orchestrate processes and data movement with Microsoft Fabric,7,Which feature helps pass values into pipeline activities?,Parameters;Themes;Fonts;Email aliases,1,false,Parameters provide dynamic values for activities.,5;1,What is the benefit of using parameters?,Reuse pipeline logic across environments and scenarios;Make code static;Hide configuration;Disable monitoring,1,Parameters make pipelines flexible and reusable.
Orchestrate processes and data movement with Microsoft Fabric,8,Which monitoring detail is important when troubleshooting a failed pipeline run?,Activity error messages and input/output;Number of slides;Wallpaper color;Inbox size,1,false,Error details show what went wrong in each activity.,5;0,Where can you find these details?,Pipeline run monitoring view;Only local logs;Clipboard;Screenshots,1,The monitoring view exposes per-activity diagnostics.
Orchestrate processes and data movement with Microsoft Fabric,9,Which scenario is ideal for orchestrating multiple technologies together?,Running Dataflows - Spark notebooks and Copy Data in sequence;Only editing a Power BI report;Changing user passwords;Scheduling meetings;Running a single technology in isolation;Combining data tasks with email management,1,false,Pipelines orchestrate heterogeneous data tasks.,5;0,Why is orchestration important in end-to-end solutions?,Ensures correct order and dependencies;Improves data visualization;Manages user permissions;Handles communication tasks,1,
Orchestrate processes and data movement with Microsoft Fabric,10,Which practice improves reliability of pipelines?,Implement retries and error handling for key activities;Disable all checks;Ignore failures;Delete logs,1,false,Retries and error handling reduce impact of transient issues.,5;0,Which activity can help route execution based on success or failure?,If Condition or Switch;Copy Data only;Spark only;KQL only,1,Control activities route logic based on outcome.
Orchestrate processes and data movement with Microsoft Fabric,11,Which pattern helps separate dev and prod pipelines?,Use different workspaces and parameterized connections;Edit prod directly;Use a single pipeline for all;Copy prod to desktop,1,false,Separate workspaces and parameters support safe environment separation.,5;0,How can you promote pipeline changes?,Export/import or recreate with same logic in prod after testing;Edit prod first;Skip testing;Use only screenshots,1,Promotion after testing protects production workloads.
Orchestrate processes and data movement with Microsoft Fabric,12,Which activity is used to run a Spark job from a pipeline?,Notebook or Spark job activity;Copy Data only;Email activity;PowerPoint activity,1,false,Notebook/Spark job activities invoke Spark workloads.,5;1,Which scenario is suitable for such an activity?,Transform Bronze to Silver using Spark;Change slide designs;Manage calendars;Rename icons,1,Spark activities in pipelines automate medallion transformations.
Orchestrate processes and data movement with Microsoft Fabric,13,Which concept helps group related pipelines and artifacts?,Workspaces as logical containers;Desktop folders;Email groups;Browser tabs,1,false,"Workspaces group related datasets, pipelines, and lakehouses.",5;0,Why is grouping by domain or project helpful?,Improves governance and discoverability;Hides assets;Increases confusion;Removes security,1,Domain-based workspaces clarify ownership and access.
Organize a Fabric lakehouse using medallion architecture design,5,Which criterion is commonly used to move data from Bronze to Silver?,Data has been cleaned and standardized;Data is still raw;Data is only screenshots;Data is only emails;Data is partially processed,1,false,Silver holds cleaned and standardized data.,3;2,What is often added in Silver compared to Bronze?,Data quality rules - standardized schemas;More noise;Random fields;Less metadata;Unstructured data,1,Silver applies quality checks and common structures.
Organize a Fabric lakehouse using medallion architecture design,6,Which type of transformation is typical when moving from Silver to Gold?,Business aggregations and calculations;Raw ingestion;Screenshot creation;Email formatting;Data duplication,1,false,Gold adds business logic and aggregations for analytics.|5,4;0,Who are primary consumers of Gold data?,Business analysts - BI tools;Only Spark admins;Only HR;Only network admins;Only developers,1,Gold is tailored for reporting and business consumption.
Organize a Fabric lakehouse using medallion architecture design,7,Which practice helps link multiple fact tables consistently across layers?,Use conformed dimensions shared between models;Duplicate attributes everywhere;Use random keys;Avoid dimensions;Use separate schemas,1,false,Conformed dimensions provide consistent keys and attributes.,4;0,At which layer are conformed dimensions typically refined?,Silver and Gold;Bronze only;Dashboard;Inbox;All layers,1,"Silver/Gold hold conformed, business-ready dimensions."
Organize a Fabric lakehouse using medallion architecture design,8,Which is a common anti-pattern in medallion architecture?,Writing reports directly on raw Bronze data;Using curated Gold;Maintaining clear layers;Documenting flows;Skipping layers,1,false,Reporting directly on Bronze bypasses quality and governance.,4;1,What is a better alternative for reporting?,Use Gold - well-defined Silver tables;Use log files only;Use unvalidated dumps;Use screenshots;Direct Bronze access,1,Curated layers ensure reliable and trusted reporting.
Organize a Fabric lakehouse using medallion architecture design,9,Which aspect of cost management is influenced by medallion design?,How much data remains in hot - detailed storage vs aggregated layers;Only license price;Only email volume;Only UI color;Only hardware costs,1,false,Layering lets you aggregate and archive to reduce storage and compute.,4;0,Which layer is a good candidate for long-term summarized storage?,Gold or separate summary layer;Raw Bronze only;Temp staging;Inbox;Silver only,1,
Organize a Fabric lakehouse using medallion architecture design,10,Which role often defines what belongs in Gold?,Business stakeholders - data product owners;Only DBAs;Only HR;Only IT security;Only data engineers,1,false,Gold reflects business definitions agreed with stakeholders.,4;1,Why is alignment with business definitions important?,Ensures reports match how the business measures KPIs;Adds confusion;Removes trust;Hides logic;Increases errors,1,Alignment improves trust and usefulness of analytics.
Organize a Fabric lakehouse using medallion architecture design,11,Which tool combination is typical for implementing medallion flows in Fabric?,Pipelines plus Spark/Dataflows plus lakehouse tables;Only Excel;Only email;Only PowerPoint;Only manual scripts,1,false,Pipelines orchestrate transformations implemented with Spark or Dataflows into lakehouse layers.,4;0,Which artifact usually represents each layer physically?,Named folders - tables for Bronze - Silver - Gold;Single undifferentiated folder;Random files;Screenshots;Separate databases,1,
Organize a Fabric lakehouse using medallion architecture design,12,Which practice supports auditability across medallion layers?,Keeping lineage and documentation of transformations;Deleting all logs;Avoiding documentation;Using only verbal notes;No tracking,1,false,Lineage and docs show how data moves and changes across layers.,4;0,Which Fabric features can help capture lineage?,Lineage views - workspace documentation;Email headers only;Clipboard;Sticky notes;Manual logs,1,Lineage views and docs help track dependencies.
Organize a Fabric lakehouse using medallion architecture design,13,Which factor should guide the granularity of Gold tables?,Reporting - analytics requirements;Random choices;Only file size;Email volume;Cost only,1,false,Choose grain to match how users consume data.,4;0,What can you do if detailed data is still needed?,Keep detailed Silver - Bronze for drill-through;Delete details;Only aggregate;Use screenshots;Store in separate system,1,Lower layers retain details for deeper analysis when needed.
